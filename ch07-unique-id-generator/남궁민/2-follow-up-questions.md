# 면접 꼬리 질문 (7장)

<details>
<summary>Q1. 왜 UUID 방식이 아닌 트위터 snowflake 방식을 선택하셨을까요?</summary>

**UUID의 한계**:

1. **정렬 불가능**:
   - UUID v4는 랜덤하게 생성되어 시간 순서 정렬 불가
   - 데이터베이스 인덱스 성능 저하 (B-tree 인덱스에서 랜덤 삽입은 페이지 분할 유발)
   - 시간 단위 쿼리에 비효율적

2. **크기 문제**:
   - UUID는 128비트로 저장 공간이 큼
   - 문자열로 표현 시 36자
   - 네트워크 전송 및 저장 비용 증가

3. **가독성 부족**:
   - 사람이 읽기 어려움
   - 디버깅 시 불편

**Snowflake의 장점**:

1. **시간 순서 보장**:
   - ID에 타임스탬프가 포함되어 시간 순서대로 정렬 가능
   - 데이터베이스 인덱스 효율적 (순차 삽입)
   - 시간 범위 쿼리 최적화

2. **컴팩트한 크기**:
   - 64비트 정수 (8바이트)
   - 숫자로 표현 가능하여 저장 효율적

3. **분산 생성 가능**:
   - 데이터센터 ID와 워커 ID로 충돌 없이 분산 생성
   - 중앙 조정자 불필요

4. **정보 포함**:
   - 타임스탬프, 데이터센터 ID, 워커 ID, 시퀀스 번호 등 메타데이터 포함
   - ID만으로도 생성 시간과 출처 파악 가능

**트레이드오프**:
- Snowflake는 시계 동기화 필요
- UUID는 완전히 독립적이지만 성능과 저장 효율성 측면에서 불리

**결론**: 대규모 분산 시스템에서 정렬 가능하고 효율적인 ID가 필요한 경우 Snowflake가 더 적합합니다.
</details>

<details>
<summary>Q2. 데이터베이스의 auto_increment를 사용하지 않는 이유는 무엇인가요?</summary>

**`auto_increment`의 한계**:

1. **단일 장애점(SPOF)**:
   - 단일 데이터베이스 서버에 의존
   - 서버 다운 시 ID 생성 불가
   - 병목 현상 발생

2. **확장성 문제**:
   - 수평 확장(scale-out) 어려움
   - 여러 데이터베이스 샤드 사용 시 ID 충돌 가능
   - 중앙 집중식 구조로 성능 한계

3. **네트워크 지연**:
   - ID 생성마다 데이터베이스 요청 필요
   - 네트워크 왕복 시간(RTT) 오버헤드
   - 높은 처리량(throughput) 요구 시 성능 저하

4. **데이터베이스 부하**:
   - 모든 ID 생성 요청이 DB로 집중
   - 쓰기 작업 증가로 락 경합 발생
   - 다른 쿼리 성능 영향

**`auto_increment`가 적합한 경우**:
- 소규모 애플리케이션
- 단일 데이터베이스로 충분한 경우
- 초당 수백 건 이하의 ID 생성

**대규모 시스템에서는 분산 ID 생성기가 필수적입니다.**
</details>

<details>
<summary>Q3. Snowflake에서 ID를 생성하는 서버들이 물리적으로 분리되어 있어서 서로 다른 시계를 사용한다고 했을 때, 어떻게 "시계 동기화(clock synchronization)"를 달성할 수 있을까요?</summary>

**시계 동기화의 중요성**:
- Snowflake ID의 타임스탬프 부분이 부정확하면 시간 순서 보장 불가
- 시계가 뒤로 가면(clock drift backward) ID 중복 가능성

**시계 동기화 방법**:

1. **NTP (Network Time Protocol)**:
   - **개념**: 네트워크를 통해 시계 동기화하는 표준 프로토콜
   - **정확도**: 일반적으로 수 밀리초 이내
   - **설정**:
     ```bash
     # NTP 서버 설정
     ntpd -gq
     ntpdate -u time.google.com
     ```
   - **장점**: 널리 사용되고 안정적
   - **단점**: 네트워크 지연에 영향 받음

2. **PTP (Precision Time Protocol)**:
   - **개념**: NTP보다 정밀한 시계 동기화 (IEEE 1588 표준)
   - **정확도**: 마이크로초 이내
   - **사용 환경**: 하드웨어 지원 필요 (특수 네트워크 스위치)
   - **장점**: 매우 높은 정밀도
   - **단점**: 비용이 높고 설정 복잡

3. **GPS 기반 동기화**:
   - **개념**: GPS 신호를 이용한 시계 동기화
   - **정확도**: 나노초 단위
   - **장점**: 외부 네트워크 불필요, 매우 정확
   - **단점**: GPS 수신기 하드웨어 필요

4. **Amazon Time Sync Service / Google TrueTime**:
   - **Amazon Time Sync**: NTP 기반, AWS 인프라에 최적화
   - **Google TrueTime**: GPS와 원자시계 조합, 시간 불확실성 범위 제공
   - **장점**: 클라우드 환경에서 관리 불필요
   - **단점**: 특정 클라우드 벤더에 종속

**Clock Drift 대응 방법**:

1. **대기(Wait) 전략**:
   - 시계가 정상으로 돌아올 때까지 대기
   - 짧은 시간(수 밀리초)이면 허용 가능

2. **워커 종료 및 재시작**:
   - 시계 오류가 심각하면 해당 워커 종료
   - 다른 워커가 요청 처리
   - 시계 재동기화 후 재시작

3. **Sequence 번호 활용**:
   - 같은 밀리초 내에서 시퀀스 번호로 순서 보장
   - 타임스탬프 변경 전 시퀀스 번호 최대한 활용

**모범 사례**:
- **정기 모니터링**: 시계 드리프트 감시
- **알림 설정**: 허용 범위 초과 시 알림
- **다중 시간 소스**: NTP 서버 여러 개 설정
- **정기 재동기화**: cron job으로 주기적 동기화
- **로그 기록**: 시계 이상 발생 시 로그 남기기

**결론**: NTP를 기본으로 사용하고, 클라우드 환경에서는 벤더 제공 시간 동기화 서비스를 활용하는 것이 현실적입니다. 시계 오류 감지 로직은 필수입니다.
</details>

<details>
<summary>Q4. Snowflake 방식에서 고가용성을 최대한 보장하기 위해서 어떤 조치를 취할 수 있을까요?</summary>

**고가용성을 위한 전략**:

1. **다중 ID 생성 서버 배포**:
   - **개념**: 여러 워커 서버를 독립적으로 운영
   - **장점**: 일부 서버 장애 시에도 서비스 지속
   - **구현**:
     - 각 워커에 고유한 워커 ID 할당
     - 로드 밸런서로 트래픽 분산
     - 최소 3개 이상의 워커 권장

2. **멀티 리전/멀티 AZ 배포**:
   - **개념**: 여러 데이터센터/가용 영역에 워커 분산
   - **장점**: 리전 단위 장애에도 대응 가능
   - **주의**: 데이터센터 ID와 워커 ID 조합이 전역적으로 유일해야 함

3. **헬스 체크 및 자동 페일오버**:
   - 로드 밸런서가 장애 워커 제외
   - 정상 워커로 트래픽 자동 라우팅
   - 장애 워커 자동 재시작

4. **워커 ID 풀 관리**:
   - **중앙 레지스트리 사용**:
     - ZooKeeper, etcd, Consul 같은 분산 코디네이터 활용
     - 워커 시작 시 사용 가능한 ID 자동 할당
     - 워커 종료 시 ID 반환하여 재사용
   - **장점**:
     - ID 충돌 방지
     - 동적 확장 가능
     - 워커 추가/제거 자동화

5. **모니터링 및 알림**:
   - **추적 메트릭**:
     - ID 생성 속도 (초당 생성 수)
     - 응답 시간
     - 에러율
     - 시계 드리프트
     - 시퀀스 번호 오버플로우 빈도
   - **알림 조건**:
     - 워커 다운
     - 에러율 임계치 초과
     - 시계 동기화 문제
     - 시퀀스 번호 고갈

6. **Rate Limiting 및 백프레셔**:
   - **개념**: 과부하 방지
   - **구현**:
     - 워커당 초당 최대 요청 수 제한
     - 큐 사용으로 급격한 트래픽 완충
     - Circuit Breaker 패턴 적용

7. **캐싱 및 배치 생성**:
   - ID 사전 생성으로 빠른 응답 속도 및 부하 분산 달성

8. **Graceful Degradation**:
   - **개념**: 부분 장애 시에도 제한적 서비스 제공
   - **구현**:
     - 일부 워커 장애 시 남은 워커로 서비스
     - 성능 저하는 있지만 완전 중단 방지
     - 자동 스케일링으로 추가 워커 기동

9. **데이터 영속성 및 상태 복구**:
   - **마지막 타임스탬프 저장**:
     - 워커 재시작 시 이전 타임스탬프 복구
     - 디스크 또는 분산 저장소에 주기적 저장
     - 시계 역행 방지

10. **카오스 엔지니어링**:
    - **개념**: 의도적 장애 주입으로 복원력 테스트
    - **테스트 시나리오**:
      - 랜덤 워커 종료
      - 네트워크 지연 주입
      - 시계 드리프트 시뮬레이션
      - 리전 단위 장애

**실무 체크리스트**:
- [ ] 최소 3개 이상의 워커 운영
- [ ] 멀티 AZ/리전 배포
- [ ] 헬스 체크 엔드포인트 구현
- [ ] 자동 페일오버 설정
- [ ] 모니터링 대시보드 구축
- [ ] 알림 시스템 설정
- [ ] 시계 동기화 모니터링
- [ ] 정기적인 부하 테스트
- [ ] 재해 복구 계획 수립

**결론**: 고가용성은 다중화, 모니터링, 자동화의 조합으로 달성됩니다. 특히 분산 환경에서는 워커 ID 관리와 시계 동기화가 핵심입니다.
</details>

<details>
<summary>Q5. ID 생성 속도를 모니터링하려면 어떤 메트릭을 추적해야 할까요?</summary>

**핵심 메트릭**:

1. **처리량(Throughput) 메트릭**:
   - **초당 생성 ID 수 (IDs per second)**
   - **목적**: 시스템 부하 파악, 용량 계획
   - **임계치 예시**: 설계상 최대 4096 IDs/ms = 4,096,000 IDs/sec (시퀀스 12비트 기준)

2. **지연 시간(Latency) 메트릭**:
   - **P50, P95, P99 응답 시간**
   - **목적**: 사용자 체감 성능 모니터링
   - **임계치 예시**: P99 < 10ms

3. **시퀀스 번호 오버플로우 빈도**:
   - **개념**: 동일 밀리초 내 시퀀스 번호 4096 초과 횟수
   - **목적**: 부하 급증 감지, 워커 추가 필요성 판단
   - **대응**: 오버플로우 빈번하면 워커 추가 또는 시퀀스 비트 증가 고려

4. **시계 드리프트(Clock Drift)**:
   - **NTP 오프셋**:
     ```bash
     # ntpq로 확인
     ntpq -c peers
     ```
   - **목적**: 시간 동기화 문제 조기 발견
   - **임계치**: ±100ms 초과 시 경고

5. **에러율(Error Rate)**:
   - **에러 타입별 카운트**
   - **목적**: 장애 조기 감지
   - **임계치**: 에러율 > 0.1% 시 알림

6. **워커별 부하 분산**:
   - **워커별 ID 생성 수**
   - **목적**: 부하 불균형 감지
   - **분석**: 특정 워커에 과도한 요청 집중 여부 확인

7. **가용성(Availability)**:
   - **활성 워커 수**
   - **헬스 체크 성공률**

8. **리소스 사용률**:
   - **CPU 사용률**
   - **메모리 사용률**
   - **네트워크 I/O**
   - **목적**: 리소스 병목 파악, 스케일링 결정

9. **ID 유일성 검증**:
   - **중복 ID 감지** (샘플링 기반)
   - **목적**: 설정 오류(워커 ID 중복 등) 감지
   - **구현**: 주기적으로 생성된 ID 샘플 수집 후 중복 검사

**모니터링 대시보드 구성**:

```
┌─────────────────────────────────────────────────┐
│         ID Generator Monitoring Dashboard        │
├─────────────────────────────────────────────────┤
│ Throughput:                                     │
│   ▓▓▓▓▓▓▓▓▓▓░░░░░ 850K IDs/sec (Max: 4M)      │
│                                                 │
│ Latency (P99):                                  │
│   ▓▓░░░░░░░░░░░░░ 2.3ms (Target: <10ms)       │
│                                                 │
│ Active Workers:                                 │
│   Region 1: ✓✓✓ (3/3)                         │
│   Region 2: ✓✓✗ (2/3) ⚠️                      │
│                                                 │
│ Sequence Overflow Rate:                         │
│   ▓░░░░░░░░░░░░░░ 0.02% (Low)                  │
│                                                 │
│ Clock Drift:                                    │
│   Worker 0: +5ms ✓                             │
│   Worker 1: +12ms ✓                            │
│   Worker 2: +150ms ⚠️                          │
│                                                 │
│ Error Rate:                                     │
│   ░░░░░░░░░░░░░░░ 0.001% (Healthy)             │
└─────────────────────────────────────────────────┘
```

**결론**: 핵심은 처리량, 지연시간, 에러율, 시계 드리프트를 지속적으로 모니터링하고, 임계치 초과 시 자동 알림을 통해 신속히 대응하는 것입니다. 시각화 대시보드로 전체적인 시스템 건강도를 한눈에 파악할 수 있어야 합니다.
</details>

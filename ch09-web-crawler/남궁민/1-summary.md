## 웹 크롤러 개요

- 검색 엔진에서 널리 쓰이는 기술
- 웹에서 새롭거나 갱신된 콘텐츠를 찾아내는 것이 주목적
- 콘텐츠 종류: 웹 페이지, 이미지, 비디오, PDF 파일 등
- 웹 페이지 몇 개 정도에서 시작해서 링크를 타고 들어가며 새 콘텐츠 수집
- 크롤러 용도
  - 검색 엔진 인덱싱
  - 웹 아카이빙
  - 웹 마이닝
  - 웹 모니터링

### 웹 크롤러의 기본적인 작동 순서

1. URL 집합이 입력으로 주어지면 해당 URL들의 웹 페이지 다운로드
2. 웹 페이지들로부터 URL 추출
3. 다운로드할 URL 목록에 추가하고 같은 작업 반복

## 설계 요구사항

- 검색 엔진 인덱싱 용도
- 매달 10억 개 웹 페이지 수집
  - QPS = 400page/sec
- 수집한 웹 페이지는 5년 간 저장
  - 웹 페이지 평균 크기는 500KB로 가정
  - 500TB/month 저장 용량 필요
  - 5년 간 30PB 저장 용량 필요

## 설계 및 구현

**핵심 컴포넌트**

- 시작 URL 집합
- 미수집 URL 저장소
- HTML 다운로더
- 도메인 이름 변환기
- 콘텐츠 파서
- 중복 콘텐츠 판별 장치
- 콘텐츠 저장소
- URL 추출기
- URL 필터
- URL 방문 여부 판별 장치
- URL 저장소

**작업 흐름**

1. 시작 URL 집합을 미수집 URL 저장소에 저장
2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져옴
3. HTML 다운로더는 도메인 이름 변환기를 사용해 URL 목록의 IP 주소들을 알아냄
4. HTML 다운로더는 IP 주소들에 접속해서 웹 페이지를 다운로드
5. 콘텐츠 파서는 HTML 페이지를 파싱 및 검증
6. 콘텐츠 저장소를 확인해서 중복 콘텐츠 여부 판별
7. 중복 콘텐츠가 아닌 경우 저장소에 콘텐츠를 저장하고 URL 추출기로 전달
8. URL 추출기는 HTML 페이지에서 링크들을 추출해서 URL 필터로 전달
9. 필터링 된 URL들은 URL 저장소에서 중복 저장 여부를 확인
10. 신규 URL들은 URL 저장소에 저장하고 미수집 URL 저장소에도 전달

### 시작 URL 집합

- 크롤링 출발점
- 가능한 한 많은 링크를 탐색할 수 있도록 설정해야 함
- 시작 URL에 대한 정답은 없고, 면접관이 의도한 바를 정확히 파악하는 것이 중요

### 미수집 URL 저장소 (URL frontier)

- 다운로드할 URL 저장 및 관리
- FIFO 큐 형태

### HTML Downloader & DNS Resolver

- HTML Downloader: 미수집 URL 저장소로부터 다운로드할 페이지 URL을 받아서 HTML 다운로드
- DNS Resolver: 웹 페이지 URL을 IP 주소로 변환
- HTML Downloader가 DNS Resolver를 통해 IP 주소를 알아냄

### 콘텐츠 파서

- 웹 페이지 저장 이전에 파싱 및 검증 절차 필요
- 크롤링 서버 안에 콘텐츠 파서를 구현하면 크롤링 과정이 느려지므로 독립적인 컴포넌트로 구성

### 중복 콘텐츠 판별 장치

- 웹 페이지 전체를 비교하는 것보다는, 해시 값을 비교해보는 것이 좋을 것

### 콘텐츠 저장소

- HTML 문서 보관 시스템
- 데이터 양이 너무 많으므로 대부분의 콘텐츠는 디스크에 저장
- 인기 콘텐츠는 메모리에 저장

### URL 추출기

- HTML 페이지를 파싱해서 링크들 추출
- 상대 경로를 절대 경로로 변환해야 함

### URL 필터

- 특정 콘텐츠 타입, 특정 파일 확장자, 접속 오류, 블랙리스트 URL 등을 대상에서 배제

### URL 방문 여부 판별 장치

- 블룸 필터나 해시 테이블이 주로 사용됨

### URL 저장소

- 이미 방문한 URL 저장

## 상세 설계

### BFS

- 웹은 directed graph라고 볼 수 있음
  - node = 웹 페이지
  - edge = URL
- 깊이가 얼마나 깊어질지 가늠할 수 없으므로 DFS는 잘 사용되지 않음

문제점

- 페이지 내 URL 상당수는 동일 서버의 내부 링크
  - 따라서 해당 서버는 크롤러를 impolite 크롤러로 간주하게 될 수 있음
- 표준적 BFS는 페이지 우선순위 기능을 제공하지 않음

### 미수집 URL 저장소

- BFS의 문제점들을 보완해주는 용도로 활용 가능

**예의(politeness)**

- 동일 서버에 짧은 시간 내 너무 많은 요청을 보내는 것을 삼가야 함
- 따라서 동일 서버에 대해 한 번에 한 페이지만 요청하도록 설계 필요
- 웹사이트의 hostname, 다운로드를 수행하는 worker thread 사이의 관계 유지를 통해 달성
  - 다운로드 스레드는 별도 FIFO 큐를 통해 가져온 URL만 다운로드
  - queue router: 같은 호스트 소속 URL은 언제나 같은 큐로 가도록 보장
  - mapping table: hostname과 큐 사이의 관계 보관
  - FIFO 큐
  - queue selector: 큐들을 순회하면서 큐에서 URL을 꺼내 지정된 작업 스레드에 전달
  - worker thread: 전달된 URL 다운로드

**우선순위**

- PageRank, 트래픽 양, 갱신 빈도 등 다양한 척도 기반 우선순위
- prioritizer: URL을 입력받아 우선순위 계산
- front queue: 우선순위 결정 과정 처리
- back queue: 크롤러가 예의 바르게 동작하도록 보증

**신선도**

- 웹 페이지는 수시로 추가/삭제/변경 되므로 이미 다운로드한 페이지도 주기적으로 재수집해야 함
- 그러나 모든 URL 재수집은 자원이 많이 소모됨
- 최적화 방법
  - 웹 페이지 변경 이력 활용
  - 우선순위를 활용해서 중요 페이지를 더 자주 수집

### HTML 다운로더

- HTTP 프로토콜을 통해 웹 페이지를 내려 받음
- Robot Exclusion Protocol에 대한 고려 필요

**Robots.txt**

- 크롤러는 이 파일에 적힌 규칙들을 먼저 확인해야 함
- 해당 파일을 주기적으로 다시 다운로드 받아서 캐시에 보관해야 할 것

**성능 최적화 방법들**

- 분산 크롤링: 크롤링 작업을 여러 서버에 분산
- DNS Resolver 결과 캐싱: DNS 요청은 동기적이므로, 도메인 네임과 IP 주소 관계를 주기적으로 캐싱
- 지역성: 크롤링 서버가 크롤링 대상 서버와 가까워질 수 있도록 지역별로 분산
- 짧은 타임아웃

**안정성 보장 방법들**

- 안정 해시: 다운로더 서버 분산 용도
- 크롤링 상태 기록: 장애 복구가 가능하도록 크롤링 상태 기록
- 예외 처리
- 데이터 검증

**콘텐츠 파싱 및 검증**

- 중복 콘텐츠: 해시나 체크섬을 통해 탐지
- spider trap 회피: URL 최대 길이 제한, 수작업으로 확인

## 추가 논의사항들

- CSR 페이지 파싱 방법
  - 페이지 파싱 전에 서버 측에서 동적으로 렌더링해서 해결
